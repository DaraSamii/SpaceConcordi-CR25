#!/bin/bash
#SBATCH --job-name=CR25
#SBATCH --mem=32G
#SBATCH --nodes=1
#SBATCH --ntasks=27
#SBATCH --cpus-per-task=1
#SBATCH --time=7:0:0    
#SBATCH --mail-user=darasamii@gmail.com
#SBATCH --mail-type=ALL
#SBATCH --account=def-tembelym
#SBATCH --output=submit.out



# Source the OpenFOAM environment (modify as needed)
if module avail openfoam/11 &>/dev/null; then
    module load openfoam/11
    echo "OpenFOAM v2312 module loaded successfully."
else
    op11
    echo "Using OpenFOAM op11."
fi

cd ${0%/*} || exit 1    # Run from this directory

# Source tutorial run functions
. $WM_PROJECT_DIR/bin/tools/RunFunctions

# Source tutorial clean functions
. $WM_PROJECT_DIR/bin/tools/CleanFunctions

cleanCase

rm -f ./0/*.gz

# Remove and recreate logs directory
if [ -d "logs" ]; then
    echo "Removing existing logs directory..."
    rm -r logs
fi
mkdir -p logs
echo "Logs directory created."

# Touch foam.foam for ParaView compatibility
touch foam.foam


cd ${0%/*} || exit 1    # Run from this directory

mkdir -p logs


if module avail openfoam/11 &>/dev/null; then
    numProcs=27  # Server
    divs="(3 3 3)"  # Adjust for server
else
    numProcs=4  # Local machine
    divs="(2 2 1)"  # Adjust for local machine
fi

# ============================
# Modify decomposeParDict
# ============================

echo "Updating decomposeParDict for $numProcs processors..."
foamDictionary system/decomposeParDict -entry "numberOfSubdomains" -set "$numProcs"
foamDictionary system/decomposeParDict -entry "hierarchicalCoeffs/n" -set "$divs"

#------------------------------------------------------------------------------
# Step 1: Generate base mesh
blockMesh | tee log.blockMesh;

# Step 2: Extract surface features
surfaceFeatures | tee ./logs/log.surfaceFeatures;

# Step 3: Decompose the domain for parallel processing
decomposePar -copyZero | tee ./logs/log.decomposePar;

# Step 4: Run snappyHexMesh in parallel on 4 cores
mpirun -np 4 snappyHexMesh -overwrite -parallel | tee ./logs/log.snappyHexMesh;

# Optional Step 5: Reconstruct the final mesh (combine into single mesh)
reconstructPar -constant | tee ./logs/log.reconstructPar;


rm -r processor*

#Speed steps
decomposePar | tee ./logs/log.decomposePar2;


mpirun -np $numProcs simpleFoam -parallel | tee ./logs/log.simpleFoam;

#NAVIGATE TO FILE LOCATION not in the folder
#to copy
#scp -r CR-25_updated p_hoffm@speed.encs.concordia.ca:/speed-scratch/p_hoffm/

#ssh to speed 
#ssh p_hoffm@speed.encs.concordia.ca

#removing on speed through putty
#[speed-submit] [/speed-scratch/p_hoffm] > rm -rf CR-25_updated/


#to run the script 
#[speed-submit] [/speed-scratch/p_hoffm] > sbatch open.sh
#job xxxxx submitted 

#to edit the script
#> nano open.sh


#to retrieve data from speed

# scp -r file-name /home/p/p_hoffm/


#to copy open.sh around
# cp /speed-scratch/p_hoffm/CR-25_updated/open.sh /speed-scratch/p_hoffm/
#            location of file currently            where you want to copy too   




#script:

#!/encs/bin/tcsh

#SBATCH --mem=20G
#SBATCH --ntasks-per-node=4
#SBATCH --ntasks=4
#SBATCH --mail-type=ALL
#SBTACH --mail-user pdh2409@gmail.com


#module load OpenFOAM/11.0/default

#echo $FOAM_RUN

#blockMesh;
#surfaceFeatures;
#decomposePar -copyZero;
#mpirun -np 4 snappyHexMesh -parallel -overwrite;
#mpirun -np 4 foamRun;
#reconstructPar;


#to follow live calculations do tail -f slurm.xxxxx.log





#to have more cpus or gpus email rt-ex-hpc@encs.concordia.ca











#check available modules
#> module avail

#> module load OpenFOAM/11.0/default

#to verify 
#> module list
